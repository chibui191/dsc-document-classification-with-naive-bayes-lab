{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with Naive Bayes - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, you'll practice implementing the Naive Bayes algorithm on your own.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will:  \n",
    "\n",
    "* Implement document classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset\n",
    "\n",
    "To start, import the dataset stored in the text file `'SMSSpamCollection'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
    "df.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "Now implement a train-test split on the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=19)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for class imbalance\n",
    "\n",
    "To help your algorithm perform more accurately, subset the dataset so that the two classes are of equal size. To do this, keep all of the instances of the minority class (spam) and subset examples of the majority class (ham) to an equal number of examples.\n",
    "\n",
    "(This would only be applied on training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3617\n",
       "spam     562\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_df = train_df[train_df.label == 'ham'].sample(n=len(train_df[train_df.label == 'spam']), random_state=19)\n",
    "spam_df = train_df[train_df.label == 'spam']\n",
    "\n",
    "train_df_undersmpled = pd.concat([ham_df, spam_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    562\n",
       "ham     562\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_undersmpled.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_classes = dict(train_df_undersmpled.label.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spam': 0.5, 'ham': 0.5}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word frequency dictionary for each class\n",
    "\n",
    "Create a word frequency dictionary for each class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_undersmpled.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "class_word_freq = {}\n",
    "\n",
    "classes = train_df_undersmpled.label.unique()\n",
    "\n",
    "for class_ in classes:\n",
    "    temp_df = train_df_undersmpled[train_df_undersmpled.label == class_]\n",
    "    bag = {}\n",
    "    \n",
    "    for row in temp_df.index:\n",
    "        doc = temp_df['text'][row].lower()\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "            \n",
    "    class_word_freq[class_] = bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the total corpus words\n",
    "Calculate V, the total number of words in the corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5413"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "vocabulary = set()\n",
    "\n",
    "for text in train_df_undersmpled['text']:\n",
    "    for word in text.lower().split():\n",
    "        vocabulary.add(word)\n",
    "        \n",
    "V = len(vocabulary)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bag of words function\n",
    "\n",
    "Before implementing the entire Naive Bayes algorithm, create a helper function `bag_it()` to create a bag of words representation from a document's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def bag_it(doc):\n",
    "    doc = doc.lower()\n",
    "    bag = {}\n",
    "    \n",
    "    for word in doc.split():\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "        \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Naive Bayes\n",
    "\n",
    "Now, implement a master function to build a naive Bayes classifier. Be sure to use the logarithmic probabilities to avoid underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def classify_doc(doc, class_word_freq, p_classes, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    \n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    \n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = np.log(p_classes[class_])\n",
    "        for word in bag.keys():\n",
    "            num = bag[word] + 1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p += (num/denom)\n",
    "            \n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    \n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "        \n",
    "    return classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your classifier\n",
    "\n",
    "Finally, test your classifier and measure its accuracy. Don't be perturbed if your results are sub-par; industry use cases would require substantial additional preprocessing before implementing the algorithm in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4209    Or i go home first lar ü wait 4 me lor.. I put...\n",
       "5466    http//tms. widelive.com/index. wml?id=820554ad...\n",
       "2455         Left dessert. U wan me 2 go suntec look 4 u?\n",
       "2513    Hiya , have u been paying money into my accoun...\n",
       "80                                 Sorry, I'll call later\n",
       "                              ...                        \n",
       "805                             K I'll be there before 4.\n",
       "3102                         Pathaya enketa maraikara pa'\n",
       "2961                     Sir send to group mail check it.\n",
       "539     Ummmmmaah Many many happy returns of d day my ...\n",
       "2398                            Neshanth..tel me who r u?\n",
       "Name: text, Length: 1393, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.35654462790141184\n",
      "Testing Accuracy: 0.34960516870064606\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "y_hat_train = X_train.map(lambda x: classify_doc(x, class_word_freq, p_classes, V))\n",
    "y_hat_test = X_test.map(lambda x: classify_doc(x, class_word_freq, p_classes, V))\n",
    "\n",
    "train_resid = y_train == y_hat_train\n",
    "print('Training Accuracy:', train_resid.value_counts(normalize=True)[True])\n",
    "\n",
    "test_resid = y_test == y_hat_test\n",
    "print('Testing Accuracy:', test_resid.value_counts(normalize=True)[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    906\n",
       "True     487\n",
       "dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_resid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up (Optional)\n",
    "\n",
    "Rework your code into an appropriate class structure so that you could easily implement the algorithm on any given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoW():\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame()\n",
    "        self.p_classes = {}\n",
    "        self.classes = []\n",
    "        self.class_word_freq = {}\n",
    "        self.V = 0\n",
    "        \n",
    "    \n",
    "    def _p_classes(self):\n",
    "        p_classes = dict(self.df.label.value_counts(normalize=True))\n",
    "        return p_classes\n",
    "    \n",
    "    \n",
    "    def _classes(self):\n",
    "        labels = self.df.label.unique()\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def _class_word_freq(self):\n",
    "        classes = self._classes()\n",
    "        for class_ in classes:\n",
    "            temp_df = df[df.label == class_]\n",
    "            bag = {}\n",
    "\n",
    "            for row in temp_df.index:\n",
    "                doc = temp_df['text'][row].lower()\n",
    "                for word in doc.split():\n",
    "                    bag[word] = bag.get(word, 0) + 1\n",
    "\n",
    "            self.class_word_freq[class_] = bag\n",
    "        return self.class_word_freq\n",
    "    \n",
    "    \n",
    "    def _vocabulary(self):\n",
    "        vocab = set()\n",
    "\n",
    "        for text in self.df.text:\n",
    "            for word in text.lower().split():\n",
    "                vocabulary.add(word)\n",
    "\n",
    "        return len(vocab)\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        # concat X and y to get 1 dataframe\n",
    "        self.df = df\n",
    "        \n",
    "        # get P for each class\n",
    "        self.p_classes = self._p_classes()\n",
    "        self.classes = self._classes()\n",
    "        \n",
    "        # get word frequency for each class\n",
    "        self.class_word_freq = self._class_word_freq()\n",
    "            \n",
    "        # calculate the number of words in corpus vocabulary\n",
    "        self.V = self._vocabulary()\n",
    "    \n",
    "    \n",
    "    def _bag_it(self, doc):\n",
    "        doc = doc.lower()\n",
    "        bag = {}\n",
    "\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "\n",
    "        return bag\n",
    "        \n",
    "    \n",
    "    def predict(self, X, return_posteriors=False):\n",
    "        preds = []\n",
    "        for doc in X:\n",
    "            # bag the document to be classified\n",
    "            bag = self._bag_it(doc)\n",
    "\n",
    "            # initiate classes & posteriors lists\n",
    "            classes = []\n",
    "            posteriors = []\n",
    "\n",
    "            # iterate through each class in class_word_freq\n",
    "            for class_ in self.class_word_freq.keys():\n",
    "                p = np.log(self.p_classes[class_])\n",
    "                for word in bag.keys():\n",
    "                    num = bag[word] + 1\n",
    "                    denom = self.class_word_freq[class_].get(word, 0) + V\n",
    "                    p += (num/denom)\n",
    "\n",
    "                classes.append(class_)\n",
    "                posteriors.append(p)\n",
    "\n",
    "            if return_posteriors:\n",
    "                print(posteriors)\n",
    "\n",
    "            preds.append(classes[np.argmax(posteriors)])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BoW()\n",
    "bow.fit(train_df_undersmpled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.872936\n",
       "True     0.127064\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_test = bow.predict(X_test)\n",
    "test_resid = y_test == y_hat_test\n",
    "test_resid.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done! In this lab, you practiced implementing Naive Bayes for document classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
